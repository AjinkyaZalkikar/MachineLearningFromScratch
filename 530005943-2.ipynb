{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5c36c5f7",
      "metadata": {
        "id": "5c36c5f7"
      },
      "source": [
        "# CSCE 633 :: Machine Learning :: Texas A&M University :: Spring 2022\n",
        "\n",
        "# Programming Assignment 4 (PA 4)\n",
        "**Name:**  Ajinkya Zalkikar\n",
        "**UIN:**  530005943"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef1b061a",
      "metadata": {
        "id": "ef1b061a"
      },
      "source": [
        "# Regression and Classification Trees\n",
        "- **100 points**\n",
        "- **Due Tuesday, April 19, 11:59 pm**\n",
        "\n",
        "In this assignment, you'll be coding up regression and classification trees from scratch. Trees are a special class of graphs with only directed edges sans any cycles. They fall under the category of directed acyclic graphs or DAGs. So, trees are DAGs where each child node has only one parent node.  \n",
        "\n",
        "Since trees are easy to design recursively, it is super important that you're familiar with **recursion**. So, it is highly recommended that you brush up on recursion and tree-based search algorithms such as depth-first search (BFS) and breadth-first search (BFS). \n",
        "\n",
        "### Instructions\n",
        "- You are **NOT** allowed to use machine learning libraries such as scikit-learn to build regression and classification trees for this assignment.\n",
        "- You are required to complete the functions defined in the code blocks following each question. Fill out sections of the code marked `\"YOUR CODE HERE\"`.\n",
        "- You're free to change the definition of the methods and the name of the methods within each class. \n",
        "- You're free to add any number of methods within each class.\n",
        "- You may also add any number of additional code blocks that you deem necessary. \n",
        "- Do **NOT REMOVE or CHANGE** any code that is **NOT** in the regressor and classifier class, and is not marked with `\"YOUR CODE HERE\"`.\n",
        "- Once you've filled out your solutions, submit the notebook on Canvas.\n",
        "- Do **NOT** forget to type in your name and UIN at the beginning of the notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a419604e",
      "metadata": {
        "id": "a419604e"
      },
      "source": [
        "Below is a suggested sequence of steps you may want to think along for building regression and classification trees.\n",
        "\n",
        "1. **Defining a criteria for splitting.**\n",
        "    1. This criteria assigns a score to a split.\n",
        "    1. For regression trees, this would be the mean squared error.\n",
        "    2. For decision trees, this would be the entropy.\n",
        "2. **Create the split.**\n",
        "    1. Split the dataset by iterating over all the rows and feature columns.\n",
        "    2. Evaluate all the splits using the splitting criteria.\n",
        "    3. Choose the best split.\n",
        "3. **Build the tree.**\n",
        "    1. Terminal nodes: decide when to stop growing a tree. This would be the maximum allowed depth of the tree or when a leaf is empty or has only 1 element.\n",
        "    2. Recursive splitting: once a split is created, you can split it further recursively by calling the same splitting function on it.\n",
        "    3. Building a tree: create a root node and apply recursive splitting on it.\n",
        "4. **Make predictions with the tree.**\n",
        "    1. For a given data point, make a prediction using the tree."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bc18eb0",
      "metadata": {
        "id": "6bc18eb0"
      },
      "source": [
        "## Question 1 (50 points)\n",
        "\n",
        "## Growing a maximum-depth regression tree\n",
        "\n",
        "The recursive procedure for growing a deep regression tree is illustrated in the figure below. We begin (on the left) by fitting a stump to the original dataset. As we move from left to right the recursion proceeds, with each leaf of the preceding tree split in order to create the next, deeper tree. As can be seen in the rightmost panel, a tree with maximum depth of four is capable of representing the training data perfectly.  \n",
        "\n",
        "![Error loading image](http://people.tamu.edu/~sumedhpendurkar/csce633/decision_tree_img.jpeg)\n",
        "\n",
        "**Peform the experiment shown in the figure by coding up a recursively defined regression tree. Instead of reproducing the plot, measure and plot the mean squared error (MSE) at each depth of your tree.**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "JPLB0gADnSws"
      },
      "id": "JPLB0gADnSws",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0ecfe6e",
      "metadata": {
        "id": "b0ecfe6e"
      },
      "outputs": [],
      "source": [
        "# Importing Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6001bc5e",
      "metadata": {
        "id": "6001bc5e"
      },
      "outputs": [],
      "source": [
        "csvname = 'http://people.tamu.edu/~sumedhpendurkar/csce633/noisy_sin_subsample_2.csv'\n",
        "data_regress = np.loadtxt(csvname, delimiter = ',')\n",
        "data_regress = np.array([[x, y] for x,y in zip(*data_regress)])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_regress"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVWD4S8SCYLN",
        "outputId": "ad70d8d1-94f5-4ceb-d66c-1588e34c643a"
      },
      "id": "hVWD4S8SCYLN",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.023528,  0.44822 ],\n",
              "       [ 0.17358 ,  0.60278 ],\n",
              "       [ 0.215   ,  0.88859 ],\n",
              "       [ 0.35531 ,  1.0266  ],\n",
              "       [ 0.41464 ,  0.67935 ],\n",
              "       [ 0.53799 , -0.10388 ],\n",
              "       [ 0.68608 , -1.1745  ],\n",
              "       [ 0.81336 , -1.1745  ],\n",
              "       [ 0.95399 , -0.34193 ]])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ff3b31f",
      "metadata": {
        "id": "8ff3b31f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "e56ffd1e-88c6-4438-df65-f208d24f0dee"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEGCAYAAABPdROvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAWX0lEQVR4nO3df5SeZX3n8feHECRVNK5Ry68YVJoVqQpOKXb3rLZqg/Y0wRUVV6tYlK4We6puTsmx1a7unpXG3W3dcqpRWbC7CtRiTLd4sggiXSuWYPjNRiJFzYQWxAY5Mq4hfveP5x4Yhsnkydzz/Jp5v86ZM8/943nuL9dJ8uG+rvu5rlQVkiTN1SGDLkCSNNoMEklSKwaJJKkVg0SS1IpBIklq5dBBFzDfVqxYUatWrRp0GZI0Um644YbvV9XT5/LeBRckq1atYtu2bYMuQ5JGSpLvzPW9dm1JkloxSCRJrRgkkqRWDBJJUisGiSSplQX31JYWh83bx9m4dQe790xw1PJlrF+zmtNPOnrQZUmLkkGikbN5+zgbLr+Fib37ABjfM8GGy28BMEykARho11aSC5Pcm+TW/RxPko8l2Znk5iQn97tGDZ+NW3c8EiKTJvbuY+PWHQOqSFrcBj1GchFw2izHXwUc3/ycA/xZH2rSkNu9Z+Kg9kvqrYEGSVVdC/xgllPWAZ+pjuuA5UmO7E91GlZHLV92UPsl9dag70gO5Gjge1O2dzX7tIitX7OaZUuXPGbfsqVLWL9m9YAqkha3BTHYnuQcOl1frFy5csDVqNcmB9R9aksaDsMeJOPAsVO2j2n2PUZVbQI2AYyNjbkI/SJw+klHGxzSkBj2rq0twFuap7dOBR6oqnsGXZQk6VEDvSNJ8jngZcCKJLuADwJLAarq48AVwKuBncBDwNsGU6kkaX8GGiRV9cYDHC/gt/tUjiRpDoa9a0uSNOQMEklSK8P+1JZ6zMkPJbVlkCxiTn4oaT7YtbWIOfmhpPlgkCxiTn4oaT4YJIuYkx9Kmg8GySLm5IeS5oOD7YuYkx9Kmg8GySLn5IeS2rJrS5LUikEiSWrFIJEktWKQSJJaMUgkSa0YJJKkVgwSSVIrBokkqRWDRJLUikEiSWrFKVIkXClSasMg0aLnSpFSO3ZtadFzpUipHYNEi54rRUrtGCRa9FwpUmrHINGi50qRUjsOtmvRc6VIqR2DZAHxEda5c6VIae4MkgXCR1glDYpjJAuEj7BKGhSDZIHwEVZJg2KQLBA+wippUAySBcJHWCUNioPtC4SPsEoaFINkAfERVkmDYNeWJKmVgQZJktOS7EiyM8l5Mxw/K8l9SW5sft4+iDolSfs3sK6tJEuAC4BXAruA65Nsqarbp516aVWd2/cCJUldGeQdySnAzqq6q6p+AlwCrBtgPZKkORhkkBwNfG/K9q5m33SvTXJzks8nObY/pUmSujXsg+1/BayqqhcAVwIXz3RSknOSbEuy7b777utrgZK02A3y8d9xYOodxjHNvkdU1f1TNj8F/NFMH1RVm4BNAGNjYzWXYpw5V5LmZpB3JNcDxyc5LslhwJnAlqknJDlyyuZa4I5eFDI5c+74ngmKR2fO3bx9/IDvlaTFbmBBUlUPA+cCW+kExGVVdVuSDyVZ25z2O0luS3IT8DvAWb2oxZlzJWnuBvrN9qq6Arhi2r4PTHm9AdjQ6zqcOVeS5m7YB9v7wplzJWnuDBKcOVeS2nDSRpw5V5LaMEgazpwrSXNj15YkqRWDRJLUikEiSWrFIJEktXLAIEny60kMHEnSjLoJiDcAdyb5oyT/vNcFSZJGywGDpKreDJwEfBu4KMnXm2nbj+h5dZKkoddVl1VV/RD4PJ1VDI8EXgN8M8m7e1ibJGkEdDNGsjbJF4BrgKXAKVX1KuCFwPt6W54kadh188321wL/taqunbqzqh5KcnZvypIkjYoDBklVvXWWY1fNbzmSpFHjY72SpFYMEklSKwaJJKmVgw6SJBcn+bMkJ/aiIEnSaJnLHcmfAl8GfmOea5EkjaCDWtiqmXNrR1VdD/xlb0qSJI2Sbr6Q+NkkT07yROBW4PYk63tfmiRpFHTTtXVCM0XK6cCXgOOwW0uS1OgmSJYmWUonSLZU1V6geluWJGlUdBMknwDuBp4IXJvkWcAPe1mUJGl0dDNFyseAj03Z9Z0kv9y7kiRJo6SbwfZnJvl0ki812ycA+51/S5K0uHTTtXURsBU4qtn+FvC7vSpIkjRaugmSFVV1GfBTgKp6GNjX06okSSOjmyD5UZKn0TypleRU4IGeViVJGhndfLP9vcAW4DlJvgY8HTijp1VJkkZGN09tfTPJS4HVQOhMkbK355VJkkbCAYMkyVum7To5CVX1mR7VJEkaId10bf3ClNeHAy8HvgkYJJKkrrq23j11O8ly4JKeVSRJGilzWY/kR3QmbpQkqasxkr/i0UkaDwFOAC6bj4snOQ34E2AJ8Kmq+si040+g04X2YuB+4A1Vdfd8XFuSND+6GSP56JTXDwPfqapdbS+cZAlwAfBKYBdwfZItVXX7lNPOBv6pqp6b5EzgfOANba8tSZo/3YyRfLVH1z4F2FlVdwEkuQRYB0wNknXAHzavPw/8aZJUldPYS9KQ2G+QJHmQmdcdCVBV9eSW1z4a+N6U7V3AL+7vnKp6OMkDwNOA70+r9RzgHICVK1e2LEuSdDD2GyRVdUQ/C2mjqjYBmwDGxsa8W5GkPupmjASAJM+g8z0SAKrquy2vPQ4cO2X7mGbfTOfsSnIo8BQ6g+6SpCHRzXoka5PcCfw98FU6qyV+aR6ufT1wfJLjkhwGnElnTq+ptvDo2idnAFc7PiJJw6Wb75F8GDgV+FZVHUfnm+3Xtb1wMx39uXTWOrkDuKyqbkvyoSRrm9M+DTwtyU46k0ee1/a6kqT51U3X1t6quj/JIUkOqaqvJPnj+bh4VV0BXDFt3wemvP4x8Lr5uJYkqTe6CZI9SZ4EXAv8zyT30vl2uyRJXQXJOmACeA/wJjoD3h/qZVGSHmvz9nE2bt3B7j0THLV8GevXrOb0k44edFkS0F2Q/BZwaVWNAxf3uB5J02zePs6Gy29hYm9nhevxPRNsuPwWAMNEQ6GbwfYjgP+d5G+SnJvkmb0uStKjNm7d8UiITJrYu4+NW3cMqCLpsQ4YJFX176vq+cBvA0cCX03y5Z5XJgmA3XsmDmq/1G8HM438vcA/0PlC4DN6U46k6Y5avuyg9kv91s0XEt+V5BrgKjrzXL2jql7Q68Ikdaxfs5plS5c8Zt+ypUtYv2b1gCqSHqubwfZjgd+tqht7XYykx5scUPepLQ2rLLQZR8bGxmrbtm2DLkOSRkqSG6pqbC7vnctSu5IkPcIgkSS10s1g+/nd7JMkLU7d3JG8coZ9r5rvQiRJo2m2pXbfCbwLeHaSm6ccOgL4Wq8LkySNhtke//0snQWs/hOPXQfkwar6QU+rkiSNjP12bVXVA1V1d1W9kc53SX6lqr4DHJLkuL5VKEkaat0Mtn8Q+D1gQ7PrMOB/9LIoSdLo6Gaw/TXAWprFrKpqN51xEkmSugqSn1Tn6+8FkOSJvS1JkjRKugmSy5J8Alie5B3Al4FP9rYsSdKoOOCkjVX10SSvBH4IrAY+UFVX9rwySdJI6Gb2X5rgMDwkSY9zwCBJ8iDN+MgUDwDbgPdV1V29KEyS1J3N28cHusxAN3ckfwzsovMFxQBnAs8BvglcCLysV8VJkma3efs4Gy6/hYm9+wAY3zPBhstvAehbmHQz2L62qj5RVQ9W1Q+rahOwpqouBZ7a4/okSbPYuHXHIyEyaWLvPjZu3dG3GroJkoeSvD7JIc3P64EfN8cW1qpYkjRidu+ZOKj9vdBNkLwJ+A3gXuAfm9dvTrIMOLeHtUmSDuCo5csOan8vzBokSZYA76qqX6+qFVX19Ob1zqqaqKr/06c6JUkzWL9mNcuWLnnMvmVLl7B+zeq+1TDrYHtV7UvyL/tVjCTp4EwOqA/7U1vbk2wB/oJmvi2Aqrq8Z1VJkrp2+klH9zU4pusmSA4H7gd+Zcq+AgwSSVJXU6S8rR+FSJJGUzffbD8cOBt4Pp27EwCq6jd7WJckaUR08/jvnwM/C6wBvgocAzzYy6IkSaNjv0GSZPJu5blV9QfAj6rqYuDXgF/sR3GSpOE32x3J3zW/9za/9yQ5EXgK8IyeViVJGhnddG1tSvJU4PeBLcDtwPltLprknyW5Msmdze8Z5+xKsi/Jjc3PljbXlCT1xmyD7c9I8t7m9eSTWxc0v9sut3secFVVfSTJec32781w3kRVvajltSRJPTRbkCwBnkRn6vjp2k7WuI5Hp5+/GLiGmYNEkjTkZguSe6rqQz267jOr6p7m9T8Az9zPeYcn2QY8DHykqjbPdFKSc4BzAFauXDnftUqSZjFbkMx0J9K1JF+m89jwdO+fulFVlWR/dzjPqqrxJM8Grk5yS1V9e/pJzRopmwDGxsac2l6S+mi2IHl5mw+uqlfs71iSf0xyZFXdk+RIOlPUz/QZ483vu5JcA5wEPC5IJEmDs9+ntqrqBz287hbgrc3rtwJfnH5CkqcmeULzegXwL+g8MSZJGiLdPP7bCx8BXpnkTuAVzTZJxpJ8qjnnecC2JDcBX6EzRmKQSNKQ6Wb233lXVfczQ9dZVW0D3t68/lvg5/tcmiTpIA3qjkSStEAYJJKkVgwSSVIrBokkqRWDRJLUikEiSWrFIJEktWKQSJJaMUgkSa0YJJKkVgwSSVIrBokkqRWDRJLUikEiSWrFIJEktWKQSJJaMUgkSa0YJJKkVgwSSVIrBokkqRWDRJLUikEiSWrFIJEktWKQSJJaMUgkSa0YJJKkVgwSSVIrBokkqRWDRJLUikEiSWrFIJEktWKQSJJaMUgkSa0YJJKkVgwSSVIrBokkqZWBBEmS1yW5LclPk4zNct5pSXYk2ZnkvH7WKEnqzqDuSG4F/jVw7f5OSLIEuAB4FXAC8MYkJ/SnPElStw4dxEWr6g6AJLOddgqws6ruas69BFgH3N7zAiVJXRvmMZKjge9N2d7V7HucJOck2ZZk23333deX4iRJHT27I0nyZeBnZzj0/qr64nxeq6o2AZsAxsbGaj4/W5I0u54FSVW9ouVHjAPHTtk+ptknSRoiw9y1dT1wfJLjkhwGnAlsGXBNkqRpBvX472uS7AJeAvx1kq3N/qOSXAFQVQ8D5wJbgTuAy6rqtkHUK0nav0E9tfUF4Asz7N8NvHrK9hXAFX0sTZJ0kIa5a0uSNAIMEklSKwaJJKkVg0SS1IpBIklqxSCRJLVikEiSWhnI90gkLS6bt4+zcesOdu+Z4Kjly1i/ZjWnnzTjHKwDNSp1DhuDRFJPbd4+zobLb2Fi7z4AxvdMsOHyWwCG6h/pUalzGNm1JamnNm7d8cg/zpMm9u5j49YdA6poZqNS5zAySCT11O49Ewe1f1BGpc5hZJBI6qmjli87qP2DMip1DiODRFJPrV+zmmVLlzxm37KlS1i/ZvWAKprZqNQ5jBxsl9RTkwPVw/401KjUOYxStbBWph0bG6tt27YNugxJGilJbqiqsbm8164tSVIrBokkqRWDRJLUikEiSWrFIJEktbLgntpKch/wnVlOWQF8v0/lDCvboMN2sA0m2Q6wuqqOmMsbF9z3SKrq6bMdT7Jtro+4LRS2QYftYBtMsh06bTDX99q1JUlqxSCRJLWyGINk06ALGAK2QYftYBtMsh1atMGCG2yXJPXXYrwjkSTNI4NEktTKgg2SJKcl2ZFkZ5LzZjj+hCSXNse/kWRV/6vsrS7a4L1Jbk9yc5KrkjxrEHX22oHaYcp5r01SSRbcY6DdtEGS1zd/Hm5L8tl+19hrXfx9WJnkK0m2N38nXj2IOnspyYVJ7k1y636OJ8nHmja6OcnJXX1wVS24H2AJ8G3g2cBhwE3ACdPOeRfw8eb1mcClg657AG3wy8DPNK/fudDaoNt2aM47ArgWuA4YG3TdA/izcDywHXhqs/2MQdc9gDbYBLyzeX0CcPeg6+5BO/wr4GTg1v0cfzXwJSDAqcA3uvnchXpHcgqws6ruqqqfAJcA66adsw64uHn9eeDlSdLHGnvtgG1QVV+pqoeazeuAY/pcYz9082cB4MPA+cCP+1lcn3TTBu8ALqiqfwKoqnv7XGOvddMGBTy5ef0UYHcf6+uLqroW+MEsp6wDPlMd1wHLkxx5oM9dqEFyNPC9Kdu7mn0znlNVDwMPAE/rS3X90U0bTHU2nf8TWWgO2A7N7fuxVfXX/Sysj7r5s/BzwM8l+VqS65Kc1rfq+qObNvhD4M1JdgFXAO/uT2lD5WD/3QAW4BQpOnhJ3gyMAS8ddC39luQQ4L8AZw24lEE7lE731svo3Jlem+Tnq2rPQKvqrzcCF1XVf07yEuDPk5xYVT8ddGHDbqHekYwDx07ZPqbZN+M5SQ6lcyt7f1+q649u2oAkrwDeD6ytqv/Xp9r66UDtcARwInBNkrvp9AtvWWAD7t38WdgFbKmqvVX198C36ATLQtFNG5wNXAZQVV8HDqczmeNi0tW/G9Mt1CC5Hjg+yXFJDqMzmL5l2jlbgLc2r88Arq5mtGmBOGAbJDkJ+ASdEFlofeKTZm2HqnqgqlZU1aqqWkVnrGhtVc15Arsh1M3fh8107kZIsoJOV9dd/Syyx7ppg+8CLwdI8jw6QXJfX6scvC3AW5qnt04FHqiqew70pgXZtVVVDyc5F9hK52mNC6vqtiQfArZV1Rbg03RuXXfSGXw6c3AVz78u22Aj8CTgL5rnDL5bVWsHVnQPdNkOC1qXbbAV+NUktwP7gPVVtWDu0Ltsg/cBn0zyHjoD72ctsP+5JMnn6PwPw4pmLOiDwFKAqvo4nbGhVwM7gYeAt3X1uQusnSRJfbZQu7YkSX1ikEiSWjFIJEmtGCSSpFYMEklSKwaJFr0k+5LcOOVn1Rw+4/QkJ8x/ddLwW5DfI5EO0kRVvajlZ5wO/C/g9m7fkOTQZp43aaR5RyLNIMmLk3w1yQ1Jtk7OgJrkHUmuT3JTkr9M8jNJfglYC2xs7miek+SayWlWkqxopl8hyVlJtiS5GrgqyRObNSL+rlkHY11z3vObfTc260Lsd7qSJE9p1tlY3Wx/Lsk7ettC0qMMEgmWTenW+kKSpcB/A86oqhcDFwL/sTn38qr6hap6IXAHcHZV/S2dqSXWV9WLqurbB7jeyc1nv5TOPGdXV9UpdNaH2ZjkicC/Bf6kuVMaozMX1oyq6gHgXOCiJGfSWVPkk3NrCung2bUlTevaSnIinYkcr2ymjlkCTM43dGKS/wAspzO9zNY5XO/KqppcE+JXgbVJ/l2zfTiwEvg68P4kx9AJrztn+8CqujLJ64ALgBfOoSZpzgwS6fEC3FZVL5nh2EXA6VV1U5KzaCY6nMHDPHrHf/i0Yz+adq3XVtWOaefckeQbwK8BVyT5raq6er8Fd6bDfx6d+ZGeyix3MNJ8s2tLerwdwNObNSlIsjTJ85tjRwD3NN1fb5ryngebY5PuBl7cvD5jlmttBd49uTpnMyMzSZ4N3FVVHwO+CLyg2X9VkpkWGnoPna62fwP896Y+qS8MEmmaZinWM4Dzk9wE3Aj8UnP4D4BvAF8D/u+Ut10CrG8GzJ8DfBR4Z5LtzL6mxYfpzL56c5Lbmm2A1wO3JrmRTjfbZ5q7jucybanUZpD97cD7qupv6Kw9//tz+o+X5sDZf6UR0Yzd/GZVvXfQtUhTGSSSpFbs2pIktWKQSJJaMUgkSa0YJJKkVgwSSVIrBokkqZX/D3iFMka26EzCAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Plot the data\n",
        "plt.figure()\n",
        "plt.scatter(data_regress[:, 0], data_regress[:, 1])\n",
        "plt.xlabel(\"Features, x\")\n",
        "plt.ylabel(\"Target values, y\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f8993ec",
      "metadata": {
        "id": "5f8993ec"
      },
      "source": [
        "**Build the regression tree in the `TreeRegressor` class**.  \n",
        "***TIP:*** *If you are smart about building the regression tree, you can reuse most of the code for building the classification tree in Question 2.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "275fdf6f",
      "metadata": {
        "id": "275fdf6f"
      },
      "outputs": [],
      "source": [
        "from posixpath import split\n",
        "class Node():\n",
        "    def __init__(self,feature_index=None,limit=None,left2=None,right2=None,varred=None,value=None):\n",
        "      self.feature_index=feature_index\n",
        "      self.limit=limit\n",
        "      self.left2=left2\n",
        "      self.right2=right2\n",
        "      self.varred=varred\n",
        "      self.value=value\n",
        "\n",
        "class TreeRegressor():\n",
        "    def __init__(self, data, max_depth=1):\n",
        "        self.data = data # last element of each row in data is the target variable\n",
        "        self.max_depth = max_depth # maximum depth\n",
        "        ######################\n",
        "        #   YOUR CODE HERE   #\n",
        "        ######################\n",
        "        # You may add additional fields\n",
        "        self.root = None\n",
        "        self.X = data[:,0]\n",
        "        self.y = data[:,1]\n",
        "\n",
        "    \n",
        "    def generate_tree(self):\n",
        "        \"\"\"\n",
        "        Generates a tree recursively.\n",
        "        \"\"\"\n",
        "        ######################\n",
        "        #   YOUR CODE HERE   #\n",
        "        ######################\n",
        "        samples = len(self.X)\n",
        "        num_of_features = 1\n",
        "        split_dict = {}\n",
        "        split_dict = self.split_func(self.data,samples,num_of_features)\n",
        "        if split_dict[\"var\"]>0:\n",
        "          left_tree = self.generate_tree(split_dict[\"left_data\"])\n",
        "          right_tree = self.generate_tree(split_dict[\"right_data\"])\n",
        "          return Node(split_dict[\"feature_index\"],split_dict[\"limit\"],left_tree,right_tree,split_dict[\"var\"])\n",
        "        leafval = self.calculate_leafval(self.y)\n",
        "        return Node(value=leafval)\n",
        "\n",
        "    def split_func(self,data,samples,num_of_features):\n",
        "        split_dict = {}\n",
        "        varred =-float(\"inf\")\n",
        "        featureval = self.data[:,0]\n",
        "        print(type(featureval))\n",
        "        print(featureval)\n",
        "        threshold = np.unique(featureval)\n",
        "        for limit in threshold:\n",
        "          left_data,right_data = self.split(self.X,featureval,limit)\n",
        "          if len(left_data)>0 and len(right_data)>0:\n",
        "            self.y,left_y,right_y = self.data[:1],left_data[:1],right_data[:1]\n",
        "            trialvar = self.var_reduction(self.y,left_y,right_y)\n",
        "            if trailvar > varred:\n",
        "              split_dict[\"feature_index\"]=feature_index\n",
        "              split_dict[\"limit\"]=limit\n",
        "              split_dict[\"left_data\"]=left_data\n",
        "              split_dict[\"right_data\"]=right_data\n",
        "              varred = trialvar\n",
        "        return split_dict\n",
        "\n",
        "    def split(self,X,featureval,limit):\n",
        "      left_data=[]\n",
        "      right_data=[]\n",
        "  \n",
        "\n",
        "    def var_reduction(self,parent,left,right):\n",
        "      weight_l = len(left)/len(parent)\n",
        "      weight_r = len(right)/len(parent)\n",
        "      reduction = np.var(parent) - (weight_l*np.var(left) + weight_r*np.var(right))\n",
        "      return reduction\n",
        "\n",
        "    def calculate_leafval(self,y):\n",
        "      val = np.mean(self.y)\n",
        "      return val\n",
        "\n",
        "    \n",
        "    def mean_squared_error(self, splits):\n",
        "        \"\"\"\n",
        "        Returns the mean squared error for a split dataset.\n",
        "        \n",
        "        Args:\n",
        "            splits: array containing splits\n",
        "        Returns:\n",
        "            MSE\n",
        "        \"\"\"\n",
        "        ######################\n",
        "        #   YOUR CODE HERE   #\n",
        "        ######################\n",
        "\n",
        "    def predict(self, tree, data_point):\n",
        "        \"\"\"\n",
        "        Returns the prediction for a sample data point.\n",
        "\n",
        "        Args:\n",
        "            tree: the tree object\n",
        "            data_point: a data point\n",
        "        Returns:\n",
        "            The predicted value.\n",
        "        \"\"\"\n",
        "        ######################\n",
        "        #   YOUR CODE HERE   #\n",
        "        ######################    \n",
        "        pred = [self.make_pred(x,self.root) for x in data_point]\n",
        "        if tree.value!= None: return tree.value\n",
        "        featureval = data_point[tree.feature_index]\n",
        "        if featureval<=tree.limit:\n",
        "          return self.predict(tree.left2,data_point)\n",
        "        else:\n",
        "          return self.predict(tree.right2,data_point)\n",
        "\n",
        "    def make_pred(self,data_point):\n",
        "      return self.predict(tree,data_point)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9448b6ec",
      "metadata": {
        "id": "9448b6ec"
      },
      "source": [
        "**Don't forget to create a method to make predictions using your tree. You may create a separate function for it or make it a part of the `TreeRegressor` class.** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba93af0c",
      "metadata": {
        "id": "ba93af0c"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4210437",
      "metadata": {
        "id": "e4210437"
      },
      "source": [
        "**Plot the MSE at each depth of your tree**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3db0f4b2",
      "metadata": {
        "id": "3db0f4b2"
      },
      "outputs": [],
      "source": [
        "mse_depths = []\n",
        "for depth in range(1, 5):\n",
        "    # Create a regression tree object\n",
        "    regressor = TreeRegressor(data_regress, depth)\n",
        "    # Train the tree\n",
        "    tree = regressor.generate_tree()\n",
        "    mse = 0.0\n",
        "    for data_point in data_regress:\n",
        "        mse += (data_point[-1] - predict(tree, data_point[:-1]))**2\n",
        "    mse_depths.append(mse/len(data_regress))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80f2b91d",
      "metadata": {
        "id": "80f2b91d"
      },
      "outputs": [],
      "source": [
        "# Plot the MSE\n",
        "plt.figure()\n",
        "plt.plot(mse_depths, '-o')\n",
        "plt.xlabel(\"Depth\")\n",
        "plt.ylabel(\"MSE\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "496655c9",
      "metadata": {
        "id": "496655c9"
      },
      "source": [
        "## Question 2 (50 points)\n",
        "\n",
        "## Growing a two-class classification tree\n",
        "\n",
        "The figure below shows the growth of a tree to a maximum depth of seven on a two-class classification dataset. As the tree grows, note how many parts of the input space do not change as leaves on the deeper branches become *pure*. By the time we reach a maximum depth of seven, there is considerable overfitting. \n",
        "\n",
        "![Error loading image](http://people.tamu.edu/~sumedhpendurkar/csce633/decision_tree_img_2.jpeg)\n",
        "\n",
        "**Perform the experiment shown in figure by coding up a recursively defined two-class classification tree. Instead of reproducing the plot, measure and plot the classification accuracy at each depth of your tree.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5da5319",
      "metadata": {
        "id": "c5da5319"
      },
      "outputs": [],
      "source": [
        "csvname = 'http://people.tamu.edu/~sumedhpendurkar/csce633/new_circle_data.csv'\n",
        "data_class = np.loadtxt(csvname, delimiter = ',')\n",
        "data_class = np.array([[x1, x2, y] for x1,x2,y in zip(*data_class)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e0dae6f",
      "metadata": {
        "scrolled": true,
        "id": "3e0dae6f"
      },
      "outputs": [],
      "source": [
        "# Plot the data\n",
        "plt.figure()\n",
        "plt.scatter(data_class[:, 0], data_class[:, 1], c=-data_class[:, 2], cmap='bwr')\n",
        "plt.xlabel(\"Features, x1\")\n",
        "plt.ylabel(\"Features, x2\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94ba3ba4",
      "metadata": {
        "id": "94ba3ba4"
      },
      "outputs": [],
      "source": [
        "# TreeClassifier is a derived class of TreeRegressor\n",
        "\n",
        "class TreeClassifier(TreeRegressor):\n",
        "    def generate_tree(self):\n",
        "        \"\"\"\n",
        "        Generates a tree recursively.\n",
        "        \"\"\"\n",
        "        ######################\n",
        "        #   YOUR CODE HERE   #\n",
        "        ######################\n",
        "\n",
        "    samples = len(self.X)\n",
        "        num_of_features = 1\n",
        "        split_dict = {}\n",
        "        split_dict = self.split_func(self.data,samples,num_of_features)\n",
        "        if split_dict[\"var\"]>0:\n",
        "          left_tree = self.generate_tree(split_dict[\"left_data\"])\n",
        "          right_tree = self.generate_tree(split_dict[\"right_data\"])\n",
        "          return Node(split_dict[\"feature_index\"],split_dict[\"limit\"],left_tree,right_tree,split_dict[\"var\"])\n",
        "        leafval = self.calculate_leafval(self.y)\n",
        "        return Node(value=leafval)\n",
        "\n",
        "    def split_func(self,data,samples,num_of_features):\n",
        "        split_dict = {}\n",
        "        varred =-float(\"inf\")\n",
        "        featureval = self.data[:,0]\n",
        "        print(type(featureval))\n",
        "        print(featureval)\n",
        "        threshold = np.unique(featureval)\n",
        "        for limit in threshold:\n",
        "          left_data,right_data = self.split(self.X,featureval,limit)\n",
        "          if len(left_data)>0 and len(right_data)>0:\n",
        "            self.y,left_y,right_y = self.data[:1],left_data[:1],right_data[:1]\n",
        "            trialvar = self.var_reduction(self.y,left_y,right_y)\n",
        "            if trailvar > varred:\n",
        "              split_dict[\"feature_index\"]=feature_index\n",
        "              split_dict[\"limit\"]=limit\n",
        "              split_dict[\"left_data\"]=left_data\n",
        "              split_dict[\"right_data\"]=right_data\n",
        "              varred = trialvar\n",
        "        return split_dict\n",
        "\n",
        "    def split(self,X,featureval,limit):\n",
        "      left_data=[]\n",
        "      right_data=[]\n",
        "  \n",
        "\n",
        "    \n",
        "    def entropy(self, y):\n",
        "        \"\"\"\n",
        "        Returns the entropy for a split dataset.\n",
        "        \n",
        "        Args:\n",
        "            splits: array containing splits\n",
        "        Returns:\n",
        "            Entropy\n",
        "        \"\"\"\n",
        "        ######################\n",
        "        #   YOUR CODE HERE   #\n",
        "        ######################\n",
        "        entropy = 0\n",
        "        for c in range(2):\n",
        "          p = len(y[y==c]/len(y))\n",
        "          entropy = entropy + -p*np.log2(p)\n",
        "        return entropy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fdf65dc",
      "metadata": {
        "id": "4fdf65dc"
      },
      "source": [
        "**Don't forget to create a method to make predictions using your tree.**  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a37a67da",
      "metadata": {
        "id": "a37a67da"
      },
      "outputs": [],
      "source": [
        "def predict(tree, data_point):\n",
        "    \"\"\"\n",
        "    Returns the prediction for a sample data point.\n",
        "\n",
        "    Args:\n",
        "        tree: the tree object\n",
        "        data_point: a data point\n",
        "    Returns:\n",
        "        The predicted class.\n",
        "    \"\"\"\n",
        "    ######################\n",
        "    #   YOUR CODE HERE   #\n",
        "    ######################"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "264bc194",
      "metadata": {
        "id": "264bc194"
      },
      "source": [
        "**Plot the classification accuracy at each depth of your tree.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c438299",
      "metadata": {
        "id": "2c438299"
      },
      "outputs": [],
      "source": [
        "accuracy_depths = []\n",
        "for depth in range(1, 8):\n",
        "    # Create a classification tree object\n",
        "    classifier = TreeClassifier(data_class, depth)\n",
        "    # Train the tree\n",
        "    tree = classifier.generate_tree()\n",
        "    correct = 0.0\n",
        "    for data_point in data_class:\n",
        "         correct += float(data_point[-1] == predict(tree, data_point[:-1]))\n",
        "    accuracy_depths.append(correct/len(data_class))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06fab926",
      "metadata": {
        "id": "06fab926"
      },
      "outputs": [],
      "source": [
        "# Plot the Accuracy\n",
        "plt.figure()\n",
        "plt.plot(accuracy_depths, '-o')\n",
        "plt.xlabel(\"Depth\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    },
    "colab": {
      "name": "530005943.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}