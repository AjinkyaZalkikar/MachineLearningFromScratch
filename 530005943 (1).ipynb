{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOf3cKn2d7ew"
      },
      "source": [
        "# CSCE 633 :: Machine Learning :: Texas A&M University :: Spring 2022\n",
        "\n",
        "# Programming Assignment 5 (PA 5) + Competition\n",
        "- **100 points**\n",
        "- **Due Wednesday, May 04, 11:59 pm**\n",
        "\n",
        "\n",
        "**Name:**  \n",
        "**UIN:**\n",
        "\n",
        "### Instructions\n",
        "- **NO LATE DAYS ALLOWED FOR THIS ASSIGNMENT**\n",
        "- You're free to edit this file as you like, although we highly recommend just filling the sections\n",
        "- Once you've filled out your solutions, submit the notebook on Canvas.\n",
        "- Do **NOT** forget to type in your name and UIN at the beginning of the notebook.\n",
        "- For further instructions (with using kaggle) please refer to the course webpage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPi6u7FGd7e3"
      },
      "source": [
        "# Convolutional Neural Networks\n",
        "\n",
        "In this assignment, you'll be coding up a convolutional neural network from scratch to classify images using PyTorch.  \n",
        "\n",
        "### Instructions\n",
        "- The maximum number of parameters you are allowed to use for your network is **100,000**. \n",
        "- You are required to complete the functions defined in the code blocks following each question. Fill out sections of the code marked `\"YOUR CODE HERE\"`.\n",
        "- You're free to add any number of methods within each class.\n",
        "- You may also add any number of additional code blocks that you deem necessary. \n",
        "- Once you've filled out your solutions, submit the notebook on Canvas following the instructions [here](https://people.engr.tamu.edu/guni/csce421/assignments.html).\n",
        "- Do **NOT** forget to type in your name and UIN at the beginning of the notebook.\n",
        "- Make sure the notebook runs on google colab **WITHOUT** any issues when all cells are ran sequentially (includes installation of libraries). Points might be deducted if there are any bugs present."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_a3M4lEd7e5"
      },
      "source": [
        "## Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKDagLaPjFLC"
      },
      "outputs": [],
      "source": [
        "!pip install torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCzMNSfw7KO0"
      },
      "outputs": [],
      "source": [
        "# \n",
        "# Checking if hardware acceleration enabled\n",
        "import os \n",
        "if int(os.environ['COLAB_GPU']) > 0:\n",
        "  print (\"*** GPU connected\")\n",
        "else:\n",
        "  print (\"*** No hardware acceleration: change to GPU under Runtime > Change runtime type > Hardware accelerator\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBZBZCnqjLsp"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0NCql1pd7e6"
      },
      "outputs": [],
      "source": [
        "# Importing the libraries\n",
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision.utils import make_grid\n",
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaDH8gtdd7e8"
      },
      "source": [
        "In this assignment, we will use the Fashion-MNIST dataset. Fashion-MNIST is a dataset of Zalando's article images—consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes.  \n",
        "\n",
        "### Data\n",
        "\n",
        "Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255.  \n",
        "\n",
        "### Labels\n",
        "\n",
        "Each training and test example is assigned to one of the following labels:\n",
        "\n",
        "| Label | Description |\n",
        "|-------|-------------|\n",
        "| 0     | T-shirt/top |\n",
        "| 1     | Trouser     |\n",
        "| 2     | Pullover    |\n",
        "| 3     | Dress       |\n",
        "| 4     | Coat        |\n",
        "| 5     | Sandal      |\n",
        "| 6     | Shirt       |\n",
        "| 7     | Sneaker     |\n",
        "| 8     | Bag         |\n",
        "| 9     | Ankle boot  |\n",
        "\n",
        "Fashion-MNIST is included in the `torchvision` library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z798mUqTd7e9"
      },
      "outputs": [],
      "source": [
        "from torchvision.datasets import FashionMNIST\n",
        "from torchvision.transforms import Compose, ToTensor, Normalize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Z-JXzcNd7e-"
      },
      "outputs": [],
      "source": [
        "# Transform to normalize the data and convert to a tensor\n",
        "transform = Compose([ToTensor(),\n",
        "    Normalize((0.5,), (0.5,))\n",
        "    ])\n",
        "\n",
        "# Download the data\n",
        "dataset = FashionMNIST('MNIST_data/', download = True, train = True, transform = transform)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poonGygNd7e_"
      },
      "source": [
        "**NOTE:** You may add more operations to `Compose` if you're performing data augmentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaAqD5iod7fA"
      },
      "source": [
        "## Data Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0HMLIl8d7fB"
      },
      "source": [
        "Let's take a look at the classes in our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fYWMPrqyd7fB"
      },
      "outputs": [],
      "source": [
        "print(dataset.classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFGZx9tPd7fC"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def show_example(img, label):\n",
        "    print('Label: {} ({})'.format(dataset.classes[label], label))\n",
        "    plt.imshow(img.squeeze(), cmap='Greys_r')\n",
        "    plt.axis(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KH0ujCLzd7fC"
      },
      "outputs": [],
      "source": [
        "show_example(*dataset[20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ibr_8mvGd7fD"
      },
      "outputs": [],
      "source": [
        "show_example(*dataset[20000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6E5gNKod7fD"
      },
      "source": [
        "## Question 1\n",
        "\n",
        "## Creating Training and Validation Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRST2b0Id7fD"
      },
      "source": [
        "The `split_indices` function takes in the size of the entire dataset, `n`, the fraction of data to be used as validation set, `val_frac`, and the random seed and returns the indices of the data points to be added to the validation dataset.  \n",
        "\n",
        "**Choose a suitable fraction for your validation set and experiment with the seed. Remember that the better your validation set, the higher the chances that your model would do well on the test set.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICQyVJSCd7fD"
      },
      "outputs": [],
      "source": [
        "def split_indices(n, val_frac, seed):\n",
        "    # Determine the size of the validation set\n",
        "    n_val = int(val_frac * n)\n",
        "    np.random.seed(seed)\n",
        "    # Create random permutation between 0 to n-1\n",
        "    idxs = np.random.permutation(n)\n",
        "    # Pick first n_val indices for validation set\n",
        "    return idxs[n_val:], idxs[:n_val]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Okd969PMd7fE"
      },
      "outputs": [],
      "source": [
        "######################\n",
        "#   YOUR CODE HERE   #\n",
        "######################\n",
        "val_frac =  0.25 ## Set the fraction for the validation set\n",
        "rand_seed =  221 ## Set the random seed\n",
        "\n",
        "train_indices, val_indices = split_indices(len(dataset), val_frac, rand_seed)\n",
        "print(\"#samples in training set: {}\".format(len(train_indices)))\n",
        "print(\"#samples in validation set: {}\".format(len(val_indices)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFioAgKgd7fE"
      },
      "source": [
        "Next, we make use of the built-in dataloaders in PyTorch to create iterables of our our training and validation sets. This helps in avoiding fitting the whole dataset into memory and only loads a batch of the data that we can decide. \n",
        "\n",
        "**Set the `batch_size` depending on the hardware resource (GPU/CPU RAM) you are using for the assignment.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9befIig0d7fE"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torch.utils.data.dataloader import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfk3dPrLd7fF"
      },
      "outputs": [],
      "source": [
        "######################\n",
        "#   YOUR CODE HERE   #\n",
        "######################\n",
        "batch_size = 18 ## Set the batch size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1LFDd_qbd7fF"
      },
      "outputs": [],
      "source": [
        "# Training sampler and data loader\n",
        "train_sampler = SubsetRandomSampler(train_indices)\n",
        "train_dl = DataLoader(dataset,\n",
        "                     batch_size,\n",
        "                     sampler=train_sampler)\n",
        "\n",
        "# Validation sampler and data loader\n",
        "val_sampler = SubsetRandomSampler(val_indices)\n",
        "val_dl = DataLoader(dataset,\n",
        "                   batch_size,\n",
        "                   sampler=val_sampler)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kl1TvXQId7fF"
      },
      "source": [
        "Plot images in a sample batch of data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W45yega0d7fF"
      },
      "outputs": [],
      "source": [
        "def show_batch(dl):\n",
        "    for images, labels in dl:\n",
        "        print (images.size())\n",
        "        fig, ax = plt.subplots(figsize=(10,10))\n",
        "        ax.set_xticks([]); ax.set_yticks([])\n",
        "        ax.imshow(make_grid(images, 8).permute(1, 2, 0), cmap='Greys_r')\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-bxq1y5d7fF"
      },
      "outputs": [],
      "source": [
        "show_batch(train_dl)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icIsVMdXd7fG"
      },
      "source": [
        "## Question 2\n",
        "\n",
        "## Building the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1swT2eKjd7fG"
      },
      "source": [
        "**Create your model by defining the network architecture in the `ImageClassifierNet` class.**  \n",
        "**NOTE:** The number of parameters in your network must be $\\leq$ 100,000."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZhEfH2Pd7fG"
      },
      "outputs": [],
      "source": [
        "# Import the libraries\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchinfo import summary"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary"
      ],
      "metadata": {
        "id": "XGMLe_-xYzjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8yeVBIMzd7fG"
      },
      "outputs": [],
      "source": [
        "from torch.nn.modules.pooling import MaxPool1d\n",
        "class ImageClassifierNet(nn.Module):\n",
        "    def __init__(self, n_channels=1, num_classes=10):\n",
        "        super(ImageClassifierNet, self).__init__()\n",
        "        ######################\n",
        "        #   YOUR CODE HERE   #\n",
        "        ######################\n",
        "\n",
        "        self.C1 = nn.Conv2d(1,16,kernel_size=2,padding=1)\n",
        "        self.C2 = nn.Conv2d(16,64,kernel_size=2)\n",
        "        self.C3 = nn.Conv2d(64,32,kernel_size=3)\n",
        "        self.C4 = nn.Conv2d(32,16,kernel_size=2,padding=1)\n",
        "        self.D1 = nn.Dropout(0.3)\n",
        "        self.D2 = nn.Dropout(0.6)\n",
        "        self.F1 = nn.Linear(576,72)\n",
        "        self.F2 = nn.Linear(72,num_classes)\n",
        "        self.MP1 = nn.MaxPool2d(kernel_size=3,stride=2)\n",
        "        self.MP2= nn.MaxPool2d(kernel_size=3,stride=2)\n",
        "        self.MP3= nn.MaxPool2d(kernel_size=2,stride=1)\n",
        "\n",
        "        self.POOL = nn.AdaptiveAvgPool2d((6, 6))\n",
        "        \n",
        "        \n",
        "    def forward(self, x):\n",
        "        ######################\n",
        "        #   YOUR CODE HERE   #\n",
        "        ######################\n",
        "        x = self.C1(x)\n",
        "        x=F.relu(x)\n",
        "        x=self.MP1(x)\n",
        "        x=self.C2(x)\n",
        "        x=F.relu(x)\n",
        "        x=self.MP2(x)\n",
        "        x=self.C3(x)\n",
        "        x=F.relu(x)\n",
        "        x=self.MP3(x)\n",
        "        x=self.C4(x)\n",
        "        x=F.relu(x)\n",
        "        x=self.POOL(x)\n",
        "        x=torch.flatten(x, 1)\n",
        "        x=self.D1(x)\n",
        "        x=self.F1(x)\n",
        "        x=self.D2(x)\n",
        "        x=F.relu(x)\n",
        "        x=self.F2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pb4smLb6d7fG"
      },
      "outputs": [],
      "source": [
        "model = ImageClassifierNet()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jc7MAuPnd7fG"
      },
      "source": [
        "The following code block prints your network architecture. It also shows the total number of parameters in your network (see `Total params`).  \n",
        "\n",
        "**NOTE: The total number of parameters in your model should be <= 100,000.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1uxKLwc9d7fH"
      },
      "outputs": [],
      "source": [
        "summary(model, input_size=(1, 28, 28))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(batch_size, 1, 28, 28)"
      ],
      "metadata": {
        "id": "ez0Bq9F4URvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVOq40u6d7fH"
      },
      "source": [
        "## Enable training on a GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ClLoelbdd7fH"
      },
      "outputs": [],
      "source": [
        "def get_default_device():\n",
        "    \"\"\"Use GPU if available, else CPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "\n",
        "def to_device(data, device):\n",
        "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
        "    if isinstance(data, (list,tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    return data.to(device, non_blocking=True)\n",
        "\n",
        "class DeviceDataLoader():\n",
        "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
        "    def __init__(self, dl, device):\n",
        "        self.dl = dl\n",
        "        self.device = device\n",
        "    \n",
        "    def __iter__(self):\n",
        "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
        "        for b in self.dl:\n",
        "            yield to_device(b, self.device)\n",
        "    \n",
        "    def __len__(self):\n",
        "        \"\"\"Number of batches\"\"\"\n",
        "        return len(self.dl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HUYiRiFd7fH"
      },
      "outputs": [],
      "source": [
        "device = get_default_device()\n",
        "\n",
        "train_dl = DeviceDataLoader(train_dl, device)\n",
        "val_dl = DeviceDataLoader(val_dl, device)\n",
        "\n",
        "to_device(model, device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "id": "JPLMSJOFs20k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roV44CHdd7fH"
      },
      "source": [
        "## Question 3 \n",
        "\n",
        "## Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "562yZnB8d7fI"
      },
      "source": [
        "**Complete the `train_model` function to train your model on a dataset. Tune your network architecture and hyperparameters on the validation set.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Phb4fOh-d7fI"
      },
      "outputs": [],
      "source": [
        "def train_model(n_epochs, model, train_dl, val_dl, loss_fn, opt_fn, lr):\n",
        "    \"\"\"\n",
        "    Trains the model on a dataset.\n",
        "    \n",
        "    Args:\n",
        "        n_epochs: number of epochs\n",
        "        model: ImageClassifierNet object\n",
        "        train_dl: training dataloader\n",
        "        val_dl: validation dataloader\n",
        "        loss_fn: the loss function\n",
        "        opt_fn: the optimizer\n",
        "        lr: learning rate\n",
        "    \n",
        "    Returns:\n",
        "        The trained model. \n",
        "        A tuple of (model, train_losses, val_losses, train_accuracies, val_accuracies)\n",
        "    \"\"\"\n",
        "    # Record these values the end of each epoch\n",
        "    train_losses, val_losses, train_accuracies, val_accuracies = [], [], [], []\n",
        "    \n",
        "    ######################\n",
        "    #   YOUR CODE HERE   #\n",
        "    ######################\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.\n",
        "        for batch_idx, (data, target) in enumerate(train_dl):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            opt_fn.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = loss_fn(output, target)\n",
        "            loss.backward()\n",
        "            opt_fn.step()\n",
        "            item_loss = loss.item()\n",
        "            train_loss = train_loss + item_loss\n",
        "\n",
        "            t_loss = train_loss/(batch_size*len(train_dl))\n",
        "\n",
        "        train_losses.append(t_loss)\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        train_correct = 0\n",
        "        for batch_idx, (data, target) in enumerate(train_dl):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            output = output.argmax(dim=1, keepdim=True).squeeze()\n",
        "            T_cor = (output==target).sum().item()\n",
        "            train_correct += T_cor\n",
        "\n",
        "            t_acc = (train_correct/(batch_size*len(train_dl)))\n",
        "\n",
        "        train_accuracies.append(t_acc)\n",
        "\n",
        "        if len(val_dl) > 0:\n",
        "            val_loss = 0\n",
        "            val_correct = 0\n",
        "            for batch_idx, (data, target) in enumerate(val_dl):\n",
        "                data, target = data.to(device), target.to(device)\n",
        "                output = model(data)\n",
        "                v_loss = loss_fn(output, target).item()\n",
        "                val_loss += v_loss\n",
        "                output = output.argmax(dim=1, keepdim=True).squeeze()\n",
        "\n",
        "                v_corr = (output==target).sum().item()\n",
        "          \n",
        "                val_correct += v_corr\n",
        "            \n",
        "            val_losses.append(val_loss/(batch_size*len(val_dl)))\n",
        "            val_accuracies.append(val_correct/(batch_size*len(val_dl)))\n",
        "\n",
        "            print(\"Epoch : {}, Train Loss : {:.4f}, Train Accuracy : {:.4f}, Val Loss : {:.4f}, Val Accuracy : {:.4f}\".format(epoch, train_loss/(batch_size*len(train_dl)), train_correct/(batch_size*len(train_dl)), \n",
        "                  val_loss/(batch_size*len(val_dl)), val_correct/(batch_size*len(val_dl))))\n",
        "        else:\n",
        "          print(\"Epoch : {}, Train Loss : {:.4f}, Train Accuracy : {:.4f}\".format(epoch, train_loss/(batch_size*len(train_dl)), train_correct/(batch_size*len(train_dl))))\n",
        "\n",
        "    \n",
        "    return model, train_losses, val_losses, train_accuracies, val_accuracies"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(val_dl)"
      ],
      "metadata": {
        "id": "Zz5iq-wy2evI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# input = torch.randn(16, 1, 28, 28)\n",
        "# output = torch.randn(16,)\n",
        "# pred = model(input).argmax(dim=1, keepdim=True).squeeze()\n",
        "# (pred==output).sum().item()"
      ],
      "metadata": {
        "id": "ZxOWYsstjP_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8SYcymDd7fI"
      },
      "source": [
        "**Set the maximum number of training epochs, the loss function, the optimizer, and the learning rate.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7JV-ovUd7fI"
      },
      "outputs": [],
      "source": [
        "######################\n",
        "#   YOUR CODE HERE   #\n",
        "######################\n",
        "num_epochs = 40  # Max number of training epochs\n",
        "loss_fn = nn.CrossEntropyLoss()  # Define the loss function\n",
        "opt_fn =  torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9) # Select an optimizer\n",
        "lr =  0.001  # Set the learning rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aATRusvYd7fI"
      },
      "outputs": [],
      "source": [
        "history = train_model(num_epochs, model, train_dl, val_dl, loss_fn, opt_fn, lr)\n",
        "\n",
        "# (Optional)\n",
        "# Once training is finished, save model as .pth and avoid retraining for the following blocks"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_dl), len(val_dl)"
      ],
      "metadata": {
        "id": "ePNAcFKwime7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xNY7i618T7w"
      },
      "outputs": [],
      "source": [
        "# (Optional)\n",
        "# Add necessary codes to the next block to load the model from file.\n",
        "# load model history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9d8bZAoI8QMk"
      },
      "outputs": [],
      "source": [
        "model, train_losses, val_losses, train_accuracies, val_accuracies = history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAlO7hJ3d7fI"
      },
      "source": [
        "## Plot loss and accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IgZH6-tRd7fJ"
      },
      "outputs": [],
      "source": [
        "def plot_accuracy(train_accuracies, val_accuracies):\n",
        "    \"\"\"Plot accuracies\"\"\"\n",
        "    plt.plot(train_accuracies, \"-x\")\n",
        "    plt.plot(val_accuracies, \"-o\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend([\"Training\", \"Validation\"])\n",
        "    plt.title(\"Accuracy vs. No. of epochs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5p949D2Jd7fJ"
      },
      "outputs": [],
      "source": [
        "plot_accuracy(train_accuracies, val_accuracies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLT6fa9Kd7fJ"
      },
      "outputs": [],
      "source": [
        "def plot_losses(train_losses, val_losses):\n",
        "    \"\"\"Plot losses\"\"\"\n",
        "    plt.plot(train_losses, \"-x\")\n",
        "    plt.plot(val_losses, \"-o\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend([\"Training\", \"Validation\"])\n",
        "    plt.title(\"Loss vs. No. of Epochs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MnwfDpOd7fJ"
      },
      "outputs": [],
      "source": [
        "plot_losses(train_losses, val_losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJ-p0RjYd7fJ"
      },
      "source": [
        "## Train a model on the entire dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCpqCmbed7fJ"
      },
      "outputs": [],
      "source": [
        "indices, _ = split_indices(len(dataset), 0, rand_seed)\n",
        "\n",
        "sampler = SubsetRandomSampler(indices)\n",
        "dl = DataLoader(dataset, batch_size, sampler=sampler)\n",
        "dl = DeviceDataLoader(dl, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsgMQH7cd7fK"
      },
      "source": [
        "**Set the maximum number of training epochs and the learning rate for finetuning your model.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tkTQCYu_d7fK"
      },
      "outputs": [],
      "source": [
        "######################\n",
        "#   YOUR CODE HERE   #\n",
        "######################\n",
        "num_epochs = 15 # Max number of training epochs\n",
        "lr = 0.05 # Set the learning rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "djPw-AArd7fK"
      },
      "outputs": [],
      "source": [
        "history = train_model(num_epochs, model, dl, [], loss_fn, opt_fn, lr)\n",
        "model = history[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9sHHEqHd7fK"
      },
      "source": [
        "## Check Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6C2v6JgOd7fK"
      },
      "outputs": [],
      "source": [
        "def view_prediction(img, label, probs, classes):\n",
        "    \"\"\"\n",
        "    Visualize predictions.\n",
        "    \"\"\"\n",
        "    probs = probs.cpu().numpy().squeeze()\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(figsize=(8,15), ncols=2)\n",
        "    ax1.imshow(img.resize_(1, 28, 28).cpu().numpy().squeeze(), cmap='Greys_r')\n",
        "    ax1.axis('off')\n",
        "    ax1.set_title('Actual: {}'.format(classes[label]))\n",
        "    ax2.barh(np.arange(10), probs)\n",
        "    ax2.set_aspect(0.1)\n",
        "    ax2.set_yticks(np.arange(10))\n",
        "    ax2.set_yticklabels(classes, size='small');\n",
        "    ax2.set_title('Predicted: probabilities')\n",
        "    ax2.set_xlim(0, 1.1)\n",
        "\n",
        "    plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3uyDRvW2d7fK"
      },
      "outputs": [],
      "source": [
        "# Calculate the class probabilites (log softmax) for img\n",
        "images = iter(dl)\n",
        "for imgs, labels in images:\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        # Calculate the class probabilites (log softmax) for img\n",
        "        probs = torch.nn.functional.softmax(model(imgs[0].unsqueeze(0)), dim=1)\n",
        "        # Plot the image and probabilites\n",
        "        view_prediction(imgs[0], labels[0], probs, dataset.classes)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuYB5juFd7fL"
      },
      "source": [
        "## Save the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fHgHYXcid7fL"
      },
      "outputs": [],
      "source": [
        "# Very important\n",
        "torch.save(model, 'model')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "lHVfjQ5k7wvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sbkp61bvd7fL"
      },
      "source": [
        "## Question 4\n",
        "\n",
        "## Compute accuracy on the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oSPdA1mCd7fL"
      },
      "outputs": [],
      "source": [
        "test_dataset = FashionMNIST('MNIST_data/', download = True, train = False, transform = transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVhz5ldTd7fL"
      },
      "outputs": [],
      "source": [
        "test_dl = DataLoader(test_dataset, batch_size)\n",
        "test_dl = DeviceDataLoader(test_dl, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxrohWhMd7fL"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, test_dl):\n",
        "    \"\"\"\n",
        "    Evaluates your model on the test data.\n",
        "    \n",
        "    Args:\n",
        "        model: ImageClassifierNet object\n",
        "        test_dl: test dataloader\n",
        "    \n",
        "    Returns: \n",
        "        Test accuracy.\n",
        "    \"\"\"\n",
        "    ######################\n",
        "    #   YOUR CODE HERE   #\n",
        "    ######################\n",
        "    model.eval()\n",
        "    test_correct = 0\n",
        "    for batch_idx, (data, target) in enumerate(test_dl):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        output = model(data)\n",
        "        output = output.argmax(dim=1, keepdim=True).squeeze()\n",
        "        test_correct += (output==target).sum().item()\n",
        "    \n",
        "    return test_correct/(batch_size*len(test_dl))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6NupaE4Ad7fL"
      },
      "outputs": [],
      "source": [
        "print(\"Test Accuracy = {:.4f}\".format(evaluate(model, test_dl)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLKxAsOFlcCR"
      },
      "source": [
        "## Preparing the CSV for Kaggle submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mbl7qqIoak3"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "from tqdm import tqdm\n",
        "from torchvision.io import read_image\n",
        "!wget --no-check-certificate \\\n",
        "    \"https://people.tamu.edu/~sumedhpendurkar/csce633/test_private/dataset.zip\" \\\n",
        "    -O \"/tmp/dataset.zip\"\n",
        "zip_ref = zipfile.ZipFile('/tmp/dataset.zip', 'r') #Opens the zip file in read mode\n",
        "zip_ref.extractall('/tmp') #Extracts the files into the /tmp folder\n",
        "zip_ref.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59P1JDTzg7q0"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "class PrivateImageDataset(Dataset):\n",
        "    def __init__(self, base_path, length, transform=None):\n",
        "        self.base_path = base_path\n",
        "        self.transform = transform\n",
        "        self.length = length\n",
        "\n",
        "    def __len__(self):\n",
        "        return (self.length)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.base_path + str(idx) + '.png'\n",
        "        im = Image.open(path)\n",
        "        if self.transform:\n",
        "            image = self.transform(im)\n",
        "        return image, 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ImlgJdfgt1bn"
      },
      "outputs": [],
      "source": [
        "def get_test_labels(model, dataset_size):\n",
        "    test_dataset = PrivateImageDataset('/tmp/', dataset_size, transform=transform)\n",
        "    test_dl = DataLoader(test_dataset, batch_size)\n",
        "    test_dl = DeviceDataLoader(test_dl, device)\n",
        "    preds, labels = [], []\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        \n",
        "        for i, data in tqdm(enumerate(test_dl)):\n",
        "            xb, yb = data\n",
        "            y_pred = model(xb)\n",
        "            _, y = torch.max(y_pred, dim=1)\n",
        "            preds.extend(y_pred)\n",
        "            labels.extend(y)\n",
        "    return preds, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JO7b6DNYo6o-"
      },
      "outputs": [],
      "source": [
        "def create_csv_for_kaggle(model):\n",
        "    dataset_size = 10000\n",
        "    _, labels = get_test_labels(model, dataset_size)\n",
        "    data = []\n",
        "    for i in range(len(labels)):\n",
        "        data.append([str(i) + '.png', labels[i].item()])\n",
        "    df = pd.DataFrame(data, columns=['id', 'label'])\n",
        "    df.to_csv('submission.csv',index=False)\n",
        "    return df\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eLlp-IuMolpg"
      },
      "outputs": [],
      "source": [
        "create_csv_for_kaggle(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYSP2fbFd7fM"
      },
      "source": [
        "## Tips to increase the test accuracy\n",
        "\n",
        "- **Data augmentation:** Diversifies your training set and leads to better generalization\n",
        "    - Flipping\n",
        "    - Rotation\n",
        "    - Shifting\n",
        "    - Cropping\n",
        "    - Adding noise\n",
        "    - Blurring\n",
        "    \n",
        "- **Regularization:** Reduces overfitting on the training set\n",
        "    - Early stopping\n",
        "    - Dropout\n",
        "    - $l_2$ regularization\n",
        "    - Batch normalization\n",
        "\n",
        "- **Hyperparameter tuning:**\n",
        "    - Weight initialization\n",
        "    - Learning rate\n",
        "    - Activation functions\n",
        "    - Optimizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vnjtcM3Td7fM"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Sbkp61bvd7fL",
        "qYSP2fbFd7fM"
      ],
      "name": "530005943.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}